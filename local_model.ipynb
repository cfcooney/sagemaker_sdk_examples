{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "import json \n",
    "\n",
    "from contextlib import contextmanager \n",
    "from io import BytesIO \n",
    "from tempfile import NamedTemporaryFile \n",
    "from transformers import PretrainedConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "  \n",
    "@contextmanager \n",
    "def s3_fileobj(bucket, key): \n",
    "    \"\"\"\n",
    "    Yields a file object from the filename at {bucket}/{key}\n",
    "\n",
    "    Args:\n",
    "        bucket (str): Name of the S3 bucket where you model is stored\n",
    "        key (str): Relative path from the base of your bucket, including the filename and extension of the object to be retrieved.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\") \n",
    "    obj = s3.get_object(Bucket=bucket, Key=key) \n",
    "    yield BytesIO(obj[\"Body\"].read()) \n",
    " \n",
    "def load_model(bucket, path_to_model, model_name='pytorch_model'):\n",
    "    \"\"\"\n",
    "    Load a model at the given S3 path. It is assumed that your model is stored at the key:\n",
    "\n",
    "        '{path_to_model}/{model_name}.bin'\n",
    "\n",
    "    and that a config has also been generated at the same path named:\n",
    "\n",
    "        f'{path_to_model}/config.json'\n",
    "\n",
    "    \"\"\"\n",
    "    tempfile = NamedTemporaryFile() \n",
    "    with s3_fileobj(bucket, f'{path_to_model}/{model_name}.bin') as f: \n",
    "        tempfile.write(f.read()) \n",
    " \n",
    "    with s3_fileobj(bucket, f'{path_to_model}/config.json') as f: \n",
    "        dict_data = json.load(f) \n",
    "        config = PretrainedConfig.from_dict(dict_data) \n",
    " \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(tempfile.name, config=config) \n",
    "    return model \n",
    "     \n",
    "model = load_model(sagemaker_session_bucket, 'huggingface-pytorch-training-2023-01-13-17-26-31-869/output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "role_name = role.split('/')[-1]\n",
    "\n",
    "#sagemaker_session_bucket = \"walkthrough-bucket-hf-aws\"\n",
    "sagemaker_session_bucket = session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {session.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {session.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"distilbert-base-uncased\"\n",
    "\n",
    "dataset_name = \"imdb\"\n",
    "\n",
    "s3_prefix = \"samples/datasets/imdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
